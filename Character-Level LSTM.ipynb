{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "wooden-dressing",
   "metadata": {},
   "source": [
    "# Character-level LSTM in PyTorch\n",
    "\n",
    "**In this notebook, a LSTM (Long Short Term Memory) model has been trained on text 'Anna'. The trained model is capable of predicting the next character and hence, can form words, sentences, and even paragraphs on its own.**\n",
    "\n",
    "**Notebook created by: [Aditya Manchanda](https://github.com/Aditya-1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dying-bankruptcy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bearing-demographic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter 1\n",
      "\n",
      "\n",
      "Happy families are all alike; every unhappy family is unhappy in its own\n",
      "way.\n",
      "\n",
      "Everythin\n"
     ]
    }
   ],
   "source": [
    "#Loading the data\n",
    "\n",
    "with open('data/anna.txt') as f:\n",
    "    text = f.read()\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "modern-pitch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "critical-welding",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "champion-rugby",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding the text - map each character to an integer and vice versa\n",
    "\n",
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch:ii for ii,ch in int2char.items()}\n",
    "\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "infinite-zambia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8, 39, 20, 48, 71, 80, 70, 68, 45, 60, 60, 60, 28, 20, 48, 48, 52,\n",
       "       68, 44, 20, 59, 76, 72, 76, 80, 69, 68, 20, 70, 80, 68, 20, 72, 72,\n",
       "       68, 20, 72, 76, 27, 80, 15, 68, 80, 55, 80, 70, 52, 68, 30, 18, 39,\n",
       "       20, 48, 48, 52, 68, 44, 20, 59, 76, 72, 52, 68, 76, 69, 68, 30, 18,\n",
       "       39, 20, 48, 48, 52, 68, 76, 18, 68, 76, 71, 69, 68, 73,  6, 18, 60,\n",
       "        6, 20, 52, 65, 60, 60, 61, 55, 80, 70, 52, 71, 39, 76, 18])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-majority",
   "metadata": {},
   "source": [
    "### Pre-processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "critical-ambassador",
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-hot encoding\n",
    "def one_hot_encode(arr,n_labels):\n",
    "    \n",
    "    #Initialize the encoded array\n",
    "    one_hot = np.zeros((arr.size,n_labels),dtype=np.float32)\n",
    "    \n",
    "    #Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]),arr.flatten()] = 1\n",
    "    \n",
    "    #Reshape it to get to final one-hot encoded array\n",
    "    one_hot = one_hot.reshape((*arr.shape,n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "competitive-italy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#Testing the one-hot encoding function\n",
    "test_seq = np.array([1,3,5,7])\n",
    "test_one_hot = one_hot_encode(test_seq,8)\n",
    "print(test_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informational-testament",
   "metadata": {},
   "source": [
    "### Making training mini-batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "sensitive-affair",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "    '''Create a generator that returns batches of size\n",
    "       batch_size x seq_length from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       batch_size: Batch size, the number of sequences per batch\n",
    "       seq_length: Number of encoded chars in a sequence\n",
    "    '''\n",
    "    \n",
    "    #Get the number of batches we can make\n",
    "    n_batches = arr.size // (batch_size*seq_length)\n",
    "    \n",
    "    #Total number of characters to keep from the array\n",
    "    arr = arr[:batch_size*seq_length*n_batches]\n",
    "    \n",
    "    #Reshape into batch_size rows\n",
    "    arr = arr.reshape(batch_size,-1)\n",
    "    \n",
    "    #Iterate over the batches using a window of size seq_length\n",
    "    for n in range(0,arr.shape[1],seq_length):\n",
    "        #The features\n",
    "        x = arr[:,n:n+seq_length]\n",
    "        \n",
    "        #The targets\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:,:-1], y[:,-1] = x[:,1:], arr[:,n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:,:-1], y[:,-1] = x[:,1:], arr[:,0]\n",
    "        \n",
    "        yield x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removable-endorsement",
   "metadata": {},
   "source": [
    "**Testing the implementation of above function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abandoned-albania",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = get_batches(encoded,8,50)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "heard-occurrence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[ 8 39 20 48 71 80 70 68 45 60]\n",
      " [69 73 18 68 71 39 20 71 68 20]\n",
      " [80 18 51 68 73 70 68 20 68 44]\n",
      " [69 68 71 39 80 68 14 39 76 80]\n",
      " [68 69 20  6 68 39 80 70 68 71]\n",
      " [14 30 69 69 76 73 18 68 20 18]\n",
      " [68 58 18 18 20 68 39 20 51 68]\n",
      " [57  0 72 73 18 69 27 52 65 68]]\n",
      "y\n",
      " [[39 20 48 71 80 70 68 45 60 60]\n",
      " [73 18 68 71 39 20 71 68 20 71]\n",
      " [18 51 68 73 70 68 20 68 44 73]\n",
      " [68 71 39 80 68 14 39 76 80 44]\n",
      " [69 20  6 68 39 80 70 68 71 80]\n",
      " [30 69 69 76 73 18 68 20 18 51]\n",
      " [58 18 18 20 68 39 20 51 68 69]\n",
      " [ 0 72 73 18 69 27 52 65 68 81]]\n"
     ]
    }
   ],
   "source": [
    "#Printing the first 10 items in a sequence\n",
    "print('x\\n',x[:10,:10])\n",
    "print('y\\n',y[:10,:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sorted-external",
   "metadata": {},
   "source": [
    "## Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "helpful-embassy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU...\n"
     ]
    }
   ],
   "source": [
    "#Checking if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if train_on_gpu:\n",
    "    print(\"Training on GPU...\")\n",
    "else:\n",
    "    print(\"GPU not available..Training on CPU...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "published-scene",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, n_hidden=256, n_layers=2, drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        #Creating character dictionaries\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch:ii for ii,ch in self.int2char.items()}\n",
    "        \n",
    "        #Defining the layers of the model\n",
    "        self.lstm = nn.LSTM(input_size=len(self.chars), hidden_size=self.n_hidden, num_layers=self.n_layers,\n",
    "                            batch_first=True, dropout=self.drop_prob)\n",
    "        self.dropout = nn.Dropout(p=self.drop_prob)\n",
    "        self.fc = nn.Linear(self.n_hidden,len(self.chars))\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        '''\n",
    "        Forward pass through the network.\n",
    "        These inputs are x, and the hidden state/cell state `hidden`.\n",
    "        '''\n",
    "        \n",
    "        #Get the outputs and new hidden state from the LSTM\n",
    "        r_output, hidden = self.lstm(x,hidden)\n",
    "        \n",
    "        #Pass the output through dropout layer\n",
    "        out = self.dropout(r_output)\n",
    "        \n",
    "        #Stack up LSTM outputs using view\n",
    "        out = out.contiguous().view(-1,self.n_hidden)\n",
    "        \n",
    "        #Finally pass the output through the fully-connected layer\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out,hidden\n",
    "    \n",
    "    def init_hidden(self,batch_size):\n",
    "        ''' Initializes the hidden state '''\n",
    "        #Create two new tensors with sizes n_layers x batch_size x n_hidden \n",
    "        #initialized to zero, for hidden state and cell state for LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if train_on_gpu:\n",
    "            hidden = (weight.new(self.n_layers,batch_size,self.n_hidden).zero_().cuda(),\n",
    "                      weight.new(self.n_layers,batch_size,self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers,batch_size,self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers,batch_size,self.n_hidden).zero_())\n",
    "            \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-onion",
   "metadata": {},
   "source": [
    "## Defining the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "combined-solid",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
    "    '''\n",
    "    Training a network\n",
    "    Arguments\n",
    "    ----------\n",
    "    net: CharRNN Network\n",
    "    data: text data to train our network\n",
    "    epochs: Number of epochs to train\n",
    "    batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
    "    seq_length: Number of character steps per mini-batch\n",
    "    lr: learning rate\n",
    "    clip: gradient clipping\n",
    "    val_frac: Fraction of data to hold out for validation\n",
    "    print_every: Number of steps for printing training and validation loss\n",
    "    '''\n",
    "    \n",
    "    net.train()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(net.parameters(),lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    #Creating training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if train_on_gpu:\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        #Initialize the hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x,y in get_batches(data,batch_size,seq_length):\n",
    "            counter += 1\n",
    "            \n",
    "            #One-hot encoding our data to feed into network\n",
    "            x = one_hot_encode(x,n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            if train_on_gpu:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            \n",
    "            #Creating new variables for hidden state\n",
    "            h = tuple([each.data for each in h])\n",
    "            \n",
    "            net.zero_grad()\n",
    "            \n",
    "            #Getting output from the model\n",
    "            output, h = net(inputs,h)\n",
    "            \n",
    "            #Calculating the loss and performing backpropagation\n",
    "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            optimizer.step()\n",
    "            \n",
    "            #Loss stats\n",
    "            if counter%print_every == 0:\n",
    "                #Get validation loss\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                \n",
    "                for x,y in get_batches(val_data, batch_size, seq_length):\n",
    "                    x = one_hot_encode(x,n_chars)\n",
    "                    inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    if train_on_gpu:\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                    \n",
    "                    output, val_h = net(inputs,val_h)\n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                net.train()\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1,epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arbitrary-individual",
   "metadata": {},
   "source": [
    "### Instantiating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "treated-young",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Setting the model hyperparameters\n",
    "n_hidden = 512\n",
    "n_layers = 2\n",
    "\n",
    "net = CharRNN(chars,n_hidden,n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "caring-frank",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20... Step: 10... Loss: 3.2736... Val Loss: 3.2161\n",
      "Epoch: 1/20... Step: 20... Loss: 3.1613... Val Loss: 3.1342\n",
      "Epoch: 1/20... Step: 30... Loss: 3.1481... Val Loss: 3.1220\n",
      "Epoch: 1/20... Step: 40... Loss: 3.1200... Val Loss: 3.1209\n",
      "Epoch: 1/20... Step: 50... Loss: 3.1443... Val Loss: 3.1178\n",
      "Epoch: 1/20... Step: 60... Loss: 3.1245... Val Loss: 3.1166\n",
      "Epoch: 1/20... Step: 70... Loss: 3.1092... Val Loss: 3.1154\n",
      "Epoch: 1/20... Step: 80... Loss: 3.1281... Val Loss: 3.1124\n",
      "Epoch: 1/20... Step: 90... Loss: 3.1224... Val Loss: 3.1060\n",
      "Epoch: 1/20... Step: 100... Loss: 3.1005... Val Loss: 3.0904\n",
      "Epoch: 1/20... Step: 110... Loss: 3.0752... Val Loss: 3.0559\n",
      "Epoch: 1/20... Step: 120... Loss: 3.0003... Val Loss: 2.9904\n",
      "Epoch: 1/20... Step: 130... Loss: 3.0108... Val Loss: 2.9222\n",
      "Epoch: 2/20... Step: 140... Loss: 2.8714... Val Loss: 2.8181\n",
      "Epoch: 2/20... Step: 150... Loss: 2.7687... Val Loss: 2.7933\n",
      "Epoch: 2/20... Step: 160... Loss: 2.6919... Val Loss: 2.6423\n",
      "Epoch: 2/20... Step: 170... Loss: 2.5829... Val Loss: 2.5777\n",
      "Epoch: 2/20... Step: 180... Loss: 2.5535... Val Loss: 2.5342\n",
      "Epoch: 2/20... Step: 190... Loss: 2.5053... Val Loss: 2.4922\n",
      "Epoch: 2/20... Step: 200... Loss: 2.4851... Val Loss: 2.4502\n",
      "Epoch: 2/20... Step: 210... Loss: 2.4473... Val Loss: 2.4254\n",
      "Epoch: 2/20... Step: 220... Loss: 2.4152... Val Loss: 2.3875\n",
      "Epoch: 2/20... Step: 230... Loss: 2.3983... Val Loss: 2.3599\n",
      "Epoch: 2/20... Step: 240... Loss: 2.3776... Val Loss: 2.3372\n",
      "Epoch: 2/20... Step: 250... Loss: 2.3210... Val Loss: 2.3164\n",
      "Epoch: 2/20... Step: 260... Loss: 2.3031... Val Loss: 2.2882\n",
      "Epoch: 2/20... Step: 270... Loss: 2.2972... Val Loss: 2.2701\n",
      "Epoch: 3/20... Step: 280... Loss: 2.2955... Val Loss: 2.2469\n",
      "Epoch: 3/20... Step: 290... Loss: 2.2629... Val Loss: 2.2220\n",
      "Epoch: 3/20... Step: 300... Loss: 2.2408... Val Loss: 2.2047\n",
      "Epoch: 3/20... Step: 310... Loss: 2.2147... Val Loss: 2.1885\n",
      "Epoch: 3/20... Step: 320... Loss: 2.1840... Val Loss: 2.1617\n",
      "Epoch: 3/20... Step: 330... Loss: 2.1510... Val Loss: 2.1443\n",
      "Epoch: 3/20... Step: 340... Loss: 2.1642... Val Loss: 2.1266\n",
      "Epoch: 3/20... Step: 350... Loss: 2.1597... Val Loss: 2.1104\n",
      "Epoch: 3/20... Step: 360... Loss: 2.0842... Val Loss: 2.0868\n",
      "Epoch: 3/20... Step: 370... Loss: 2.1126... Val Loss: 2.0710\n",
      "Epoch: 3/20... Step: 380... Loss: 2.0890... Val Loss: 2.0566\n",
      "Epoch: 3/20... Step: 390... Loss: 2.0516... Val Loss: 2.0374\n",
      "Epoch: 3/20... Step: 400... Loss: 2.0345... Val Loss: 2.0216\n",
      "Epoch: 3/20... Step: 410... Loss: 2.0470... Val Loss: 2.0134\n",
      "Epoch: 4/20... Step: 420... Loss: 2.0257... Val Loss: 1.9954\n",
      "Epoch: 4/20... Step: 430... Loss: 2.0159... Val Loss: 1.9777\n",
      "Epoch: 4/20... Step: 440... Loss: 1.9961... Val Loss: 1.9665\n",
      "Epoch: 4/20... Step: 450... Loss: 1.9393... Val Loss: 1.9494\n",
      "Epoch: 4/20... Step: 460... Loss: 1.9333... Val Loss: 1.9343\n",
      "Epoch: 4/20... Step: 470... Loss: 1.9599... Val Loss: 1.9260\n",
      "Epoch: 4/20... Step: 480... Loss: 1.9423... Val Loss: 1.9152\n",
      "Epoch: 4/20... Step: 490... Loss: 1.9452... Val Loss: 1.8985\n",
      "Epoch: 4/20... Step: 500... Loss: 1.9261... Val Loss: 1.8882\n",
      "Epoch: 4/20... Step: 510... Loss: 1.9096... Val Loss: 1.8730\n",
      "Epoch: 4/20... Step: 520... Loss: 1.9240... Val Loss: 1.8616\n",
      "Epoch: 4/20... Step: 530... Loss: 1.8759... Val Loss: 1.8502\n",
      "Epoch: 4/20... Step: 540... Loss: 1.8367... Val Loss: 1.8413\n",
      "Epoch: 4/20... Step: 550... Loss: 1.8839... Val Loss: 1.8286\n",
      "Epoch: 5/20... Step: 560... Loss: 1.8495... Val Loss: 1.8204\n",
      "Epoch: 5/20... Step: 570... Loss: 1.8337... Val Loss: 1.8098\n",
      "Epoch: 5/20... Step: 580... Loss: 1.8170... Val Loss: 1.8016\n",
      "Epoch: 5/20... Step: 590... Loss: 1.8131... Val Loss: 1.7892\n",
      "Epoch: 5/20... Step: 600... Loss: 1.8036... Val Loss: 1.7819\n",
      "Epoch: 5/20... Step: 610... Loss: 1.7919... Val Loss: 1.7771\n",
      "Epoch: 5/20... Step: 620... Loss: 1.7868... Val Loss: 1.7628\n",
      "Epoch: 5/20... Step: 630... Loss: 1.8011... Val Loss: 1.7558\n",
      "Epoch: 5/20... Step: 640... Loss: 1.7732... Val Loss: 1.7472\n",
      "Epoch: 5/20... Step: 650... Loss: 1.7511... Val Loss: 1.7379\n",
      "Epoch: 5/20... Step: 660... Loss: 1.7334... Val Loss: 1.7297\n",
      "Epoch: 5/20... Step: 670... Loss: 1.7475... Val Loss: 1.7218\n",
      "Epoch: 5/20... Step: 680... Loss: 1.7555... Val Loss: 1.7132\n",
      "Epoch: 5/20... Step: 690... Loss: 1.7334... Val Loss: 1.7060\n",
      "Epoch: 6/20... Step: 700... Loss: 1.7185... Val Loss: 1.6940\n",
      "Epoch: 6/20... Step: 710... Loss: 1.7116... Val Loss: 1.6936\n",
      "Epoch: 6/20... Step: 720... Loss: 1.6966... Val Loss: 1.6864\n",
      "Epoch: 6/20... Step: 730... Loss: 1.7032... Val Loss: 1.6765\n",
      "Epoch: 6/20... Step: 740... Loss: 1.6808... Val Loss: 1.6717\n",
      "Epoch: 6/20... Step: 750... Loss: 1.6526... Val Loss: 1.6689\n",
      "Epoch: 6/20... Step: 760... Loss: 1.6935... Val Loss: 1.6607\n",
      "Epoch: 6/20... Step: 770... Loss: 1.6761... Val Loss: 1.6561\n",
      "Epoch: 6/20... Step: 780... Loss: 1.6667... Val Loss: 1.6452\n",
      "Epoch: 6/20... Step: 790... Loss: 1.6486... Val Loss: 1.6430\n",
      "Epoch: 6/20... Step: 800... Loss: 1.6660... Val Loss: 1.6373\n",
      "Epoch: 6/20... Step: 810... Loss: 1.6548... Val Loss: 1.6336\n",
      "Epoch: 6/20... Step: 820... Loss: 1.6165... Val Loss: 1.6249\n",
      "Epoch: 6/20... Step: 830... Loss: 1.6544... Val Loss: 1.6211\n",
      "Epoch: 7/20... Step: 840... Loss: 1.6146... Val Loss: 1.6126\n",
      "Epoch: 7/20... Step: 850... Loss: 1.6314... Val Loss: 1.6133\n",
      "Epoch: 7/20... Step: 860... Loss: 1.6092... Val Loss: 1.6046\n",
      "Epoch: 7/20... Step: 870... Loss: 1.6186... Val Loss: 1.6005\n",
      "Epoch: 7/20... Step: 880... Loss: 1.6145... Val Loss: 1.5994\n",
      "Epoch: 7/20... Step: 890... Loss: 1.6126... Val Loss: 1.5938\n",
      "Epoch: 7/20... Step: 900... Loss: 1.5920... Val Loss: 1.5905\n",
      "Epoch: 7/20... Step: 910... Loss: 1.5742... Val Loss: 1.5844\n",
      "Epoch: 7/20... Step: 920... Loss: 1.5872... Val Loss: 1.5785\n",
      "Epoch: 7/20... Step: 930... Loss: 1.5762... Val Loss: 1.5727\n",
      "Epoch: 7/20... Step: 940... Loss: 1.5751... Val Loss: 1.5686\n",
      "Epoch: 7/20... Step: 950... Loss: 1.5853... Val Loss: 1.5680\n",
      "Epoch: 7/20... Step: 960... Loss: 1.5887... Val Loss: 1.5613\n",
      "Epoch: 7/20... Step: 970... Loss: 1.5939... Val Loss: 1.5606\n",
      "Epoch: 8/20... Step: 980... Loss: 1.5564... Val Loss: 1.5487\n",
      "Epoch: 8/20... Step: 990... Loss: 1.5646... Val Loss: 1.5496\n",
      "Epoch: 8/20... Step: 1000... Loss: 1.5497... Val Loss: 1.5424\n",
      "Epoch: 8/20... Step: 1010... Loss: 1.5879... Val Loss: 1.5368\n",
      "Epoch: 8/20... Step: 1020... Loss: 1.5533... Val Loss: 1.5397\n",
      "Epoch: 8/20... Step: 1030... Loss: 1.5354... Val Loss: 1.5363\n",
      "Epoch: 8/20... Step: 1040... Loss: 1.5463... Val Loss: 1.5344\n",
      "Epoch: 8/20... Step: 1050... Loss: 1.5245... Val Loss: 1.5289\n",
      "Epoch: 8/20... Step: 1060... Loss: 1.5284... Val Loss: 1.5274\n",
      "Epoch: 8/20... Step: 1070... Loss: 1.5322... Val Loss: 1.5206\n",
      "Epoch: 8/20... Step: 1080... Loss: 1.5290... Val Loss: 1.5177\n",
      "Epoch: 8/20... Step: 1090... Loss: 1.5205... Val Loss: 1.5142\n",
      "Epoch: 8/20... Step: 1100... Loss: 1.5097... Val Loss: 1.5087\n",
      "Epoch: 8/20... Step: 1110... Loss: 1.5139... Val Loss: 1.5048\n",
      "Epoch: 9/20... Step: 1120... Loss: 1.5272... Val Loss: 1.5042\n",
      "Epoch: 9/20... Step: 1130... Loss: 1.5179... Val Loss: 1.5002\n",
      "Epoch: 9/20... Step: 1140... Loss: 1.5155... Val Loss: 1.4979\n",
      "Epoch: 9/20... Step: 1150... Loss: 1.5295... Val Loss: 1.4941\n",
      "Epoch: 9/20... Step: 1160... Loss: 1.4686... Val Loss: 1.4935\n",
      "Epoch: 9/20... Step: 1170... Loss: 1.4923... Val Loss: 1.4898\n",
      "Epoch: 9/20... Step: 1180... Loss: 1.4839... Val Loss: 1.4859\n",
      "Epoch: 9/20... Step: 1190... Loss: 1.5131... Val Loss: 1.4847\n",
      "Epoch: 9/20... Step: 1200... Loss: 1.4672... Val Loss: 1.4804\n",
      "Epoch: 9/20... Step: 1210... Loss: 1.4858... Val Loss: 1.4770\n",
      "Epoch: 9/20... Step: 1220... Loss: 1.4745... Val Loss: 1.4774\n",
      "Epoch: 9/20... Step: 1230... Loss: 1.4621... Val Loss: 1.4732\n",
      "Epoch: 9/20... Step: 1240... Loss: 1.4697... Val Loss: 1.4700\n",
      "Epoch: 9/20... Step: 1250... Loss: 1.4663... Val Loss: 1.4685\n",
      "Epoch: 10/20... Step: 1260... Loss: 1.4745... Val Loss: 1.4658\n",
      "Epoch: 10/20... Step: 1270... Loss: 1.4666... Val Loss: 1.4613\n",
      "Epoch: 10/20... Step: 1280... Loss: 1.4841... Val Loss: 1.4586\n",
      "Epoch: 10/20... Step: 1290... Loss: 1.4649... Val Loss: 1.4576\n",
      "Epoch: 10/20... Step: 1300... Loss: 1.4561... Val Loss: 1.4576\n",
      "Epoch: 10/20... Step: 1310... Loss: 1.4673... Val Loss: 1.4540\n",
      "Epoch: 10/20... Step: 1320... Loss: 1.4270... Val Loss: 1.4544\n",
      "Epoch: 10/20... Step: 1330... Loss: 1.4462... Val Loss: 1.4511\n",
      "Epoch: 10/20... Step: 1340... Loss: 1.4242... Val Loss: 1.4452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/20... Step: 1350... Loss: 1.4213... Val Loss: 1.4496\n",
      "Epoch: 10/20... Step: 1360... Loss: 1.4195... Val Loss: 1.4409\n",
      "Epoch: 10/20... Step: 1370... Loss: 1.4090... Val Loss: 1.4399\n",
      "Epoch: 10/20... Step: 1380... Loss: 1.4538... Val Loss: 1.4395\n",
      "Epoch: 10/20... Step: 1390... Loss: 1.4568... Val Loss: 1.4342\n",
      "Epoch: 11/20... Step: 1400... Loss: 1.4608... Val Loss: 1.4335\n",
      "Epoch: 11/20... Step: 1410... Loss: 1.4626... Val Loss: 1.4343\n",
      "Epoch: 11/20... Step: 1420... Loss: 1.4588... Val Loss: 1.4300\n",
      "Epoch: 11/20... Step: 1430... Loss: 1.4142... Val Loss: 1.4282\n",
      "Epoch: 11/20... Step: 1440... Loss: 1.4451... Val Loss: 1.4291\n",
      "Epoch: 11/20... Step: 1450... Loss: 1.3768... Val Loss: 1.4239\n",
      "Epoch: 11/20... Step: 1460... Loss: 1.4080... Val Loss: 1.4219\n",
      "Epoch: 11/20... Step: 1470... Loss: 1.4004... Val Loss: 1.4219\n",
      "Epoch: 11/20... Step: 1480... Loss: 1.4173... Val Loss: 1.4200\n",
      "Epoch: 11/20... Step: 1490... Loss: 1.4099... Val Loss: 1.4198\n",
      "Epoch: 11/20... Step: 1500... Loss: 1.3912... Val Loss: 1.4183\n",
      "Epoch: 11/20... Step: 1510... Loss: 1.3707... Val Loss: 1.4166\n",
      "Epoch: 11/20... Step: 1520... Loss: 1.4118... Val Loss: 1.4126\n",
      "Epoch: 12/20... Step: 1530... Loss: 1.4578... Val Loss: 1.4122\n",
      "Epoch: 12/20... Step: 1540... Loss: 1.4167... Val Loss: 1.4063\n",
      "Epoch: 12/20... Step: 1550... Loss: 1.4231... Val Loss: 1.4061\n",
      "Epoch: 12/20... Step: 1560... Loss: 1.4230... Val Loss: 1.4037\n",
      "Epoch: 12/20... Step: 1570... Loss: 1.3692... Val Loss: 1.4055\n",
      "Epoch: 12/20... Step: 1580... Loss: 1.3462... Val Loss: 1.4049\n",
      "Epoch: 12/20... Step: 1590... Loss: 1.3491... Val Loss: 1.4022\n",
      "Epoch: 12/20... Step: 1600... Loss: 1.3803... Val Loss: 1.3983\n",
      "Epoch: 12/20... Step: 1610... Loss: 1.3698... Val Loss: 1.4023\n",
      "Epoch: 12/20... Step: 1620... Loss: 1.3671... Val Loss: 1.3968\n",
      "Epoch: 12/20... Step: 1630... Loss: 1.3882... Val Loss: 1.3961\n",
      "Epoch: 12/20... Step: 1640... Loss: 1.3686... Val Loss: 1.3950\n",
      "Epoch: 12/20... Step: 1650... Loss: 1.3440... Val Loss: 1.3932\n",
      "Epoch: 12/20... Step: 1660... Loss: 1.3981... Val Loss: 1.3900\n",
      "Epoch: 13/20... Step: 1670... Loss: 1.3557... Val Loss: 1.3934\n",
      "Epoch: 13/20... Step: 1680... Loss: 1.3764... Val Loss: 1.3878\n",
      "Epoch: 13/20... Step: 1690... Loss: 1.3564... Val Loss: 1.3858\n",
      "Epoch: 13/20... Step: 1700... Loss: 1.3618... Val Loss: 1.3868\n",
      "Epoch: 13/20... Step: 1710... Loss: 1.3271... Val Loss: 1.3854\n",
      "Epoch: 13/20... Step: 1720... Loss: 1.3436... Val Loss: 1.3867\n",
      "Epoch: 13/20... Step: 1730... Loss: 1.3895... Val Loss: 1.3808\n",
      "Epoch: 13/20... Step: 1740... Loss: 1.3508... Val Loss: 1.3842\n",
      "Epoch: 13/20... Step: 1750... Loss: 1.3204... Val Loss: 1.3832\n",
      "Epoch: 13/20... Step: 1760... Loss: 1.3511... Val Loss: 1.3820\n",
      "Epoch: 13/20... Step: 1770... Loss: 1.3558... Val Loss: 1.3772\n",
      "Epoch: 13/20... Step: 1780... Loss: 1.3404... Val Loss: 1.3778\n",
      "Epoch: 13/20... Step: 1790... Loss: 1.3343... Val Loss: 1.3759\n",
      "Epoch: 13/20... Step: 1800... Loss: 1.3515... Val Loss: 1.3759\n",
      "Epoch: 14/20... Step: 1810... Loss: 1.3516... Val Loss: 1.3811\n",
      "Epoch: 14/20... Step: 1820... Loss: 1.3424... Val Loss: 1.3705\n",
      "Epoch: 14/20... Step: 1830... Loss: 1.3524... Val Loss: 1.3696\n",
      "Epoch: 14/20... Step: 1840... Loss: 1.3003... Val Loss: 1.3688\n",
      "Epoch: 14/20... Step: 1850... Loss: 1.2932... Val Loss: 1.3666\n",
      "Epoch: 14/20... Step: 1860... Loss: 1.3427... Val Loss: 1.3687\n",
      "Epoch: 14/20... Step: 1870... Loss: 1.3512... Val Loss: 1.3642\n",
      "Epoch: 14/20... Step: 1880... Loss: 1.3415... Val Loss: 1.3642\n",
      "Epoch: 14/20... Step: 1890... Loss: 1.3541... Val Loss: 1.3614\n",
      "Epoch: 14/20... Step: 1900... Loss: 1.3369... Val Loss: 1.3619\n",
      "Epoch: 14/20... Step: 1910... Loss: 1.3380... Val Loss: 1.3647\n",
      "Epoch: 14/20... Step: 1920... Loss: 1.3293... Val Loss: 1.3600\n",
      "Epoch: 14/20... Step: 1930... Loss: 1.2964... Val Loss: 1.3593\n",
      "Epoch: 14/20... Step: 1940... Loss: 1.3516... Val Loss: 1.3574\n",
      "Epoch: 15/20... Step: 1950... Loss: 1.3251... Val Loss: 1.3630\n",
      "Epoch: 15/20... Step: 1960... Loss: 1.3194... Val Loss: 1.3554\n",
      "Epoch: 15/20... Step: 1970... Loss: 1.3133... Val Loss: 1.3576\n",
      "Epoch: 15/20... Step: 1980... Loss: 1.3079... Val Loss: 1.3602\n",
      "Epoch: 15/20... Step: 1990... Loss: 1.3096... Val Loss: 1.3522\n",
      "Epoch: 15/20... Step: 2000... Loss: 1.2965... Val Loss: 1.3515\n",
      "Epoch: 15/20... Step: 2010... Loss: 1.3093... Val Loss: 1.3499\n",
      "Epoch: 15/20... Step: 2020... Loss: 1.3282... Val Loss: 1.3536\n",
      "Epoch: 15/20... Step: 2030... Loss: 1.2989... Val Loss: 1.3488\n",
      "Epoch: 15/20... Step: 2040... Loss: 1.3147... Val Loss: 1.3519\n",
      "Epoch: 15/20... Step: 2050... Loss: 1.3009... Val Loss: 1.3500\n",
      "Epoch: 15/20... Step: 2060... Loss: 1.3094... Val Loss: 1.3489\n",
      "Epoch: 15/20... Step: 2070... Loss: 1.3225... Val Loss: 1.3455\n",
      "Epoch: 15/20... Step: 2080... Loss: 1.3081... Val Loss: 1.3454\n",
      "Epoch: 16/20... Step: 2090... Loss: 1.3100... Val Loss: 1.3493\n",
      "Epoch: 16/20... Step: 2100... Loss: 1.2891... Val Loss: 1.3443\n",
      "Epoch: 16/20... Step: 2110... Loss: 1.2940... Val Loss: 1.3504\n",
      "Epoch: 16/20... Step: 2120... Loss: 1.3135... Val Loss: 1.3481\n",
      "Epoch: 16/20... Step: 2130... Loss: 1.2747... Val Loss: 1.3453\n",
      "Epoch: 16/20... Step: 2140... Loss: 1.2915... Val Loss: 1.3414\n",
      "Epoch: 16/20... Step: 2150... Loss: 1.3130... Val Loss: 1.3391\n",
      "Epoch: 16/20... Step: 2160... Loss: 1.2869... Val Loss: 1.3412\n",
      "Epoch: 16/20... Step: 2170... Loss: 1.2877... Val Loss: 1.3397\n",
      "Epoch: 16/20... Step: 2180... Loss: 1.2845... Val Loss: 1.3422\n",
      "Epoch: 16/20... Step: 2190... Loss: 1.3095... Val Loss: 1.3422\n",
      "Epoch: 16/20... Step: 2200... Loss: 1.2889... Val Loss: 1.3358\n",
      "Epoch: 16/20... Step: 2210... Loss: 1.2449... Val Loss: 1.3343\n",
      "Epoch: 16/20... Step: 2220... Loss: 1.2940... Val Loss: 1.3364\n",
      "Epoch: 17/20... Step: 2230... Loss: 1.2663... Val Loss: 1.3373\n",
      "Epoch: 17/20... Step: 2240... Loss: 1.2854... Val Loss: 1.3330\n",
      "Epoch: 17/20... Step: 2250... Loss: 1.2553... Val Loss: 1.3368\n",
      "Epoch: 17/20... Step: 2260... Loss: 1.2767... Val Loss: 1.3355\n",
      "Epoch: 17/20... Step: 2270... Loss: 1.2871... Val Loss: 1.3326\n",
      "Epoch: 17/20... Step: 2280... Loss: 1.2878... Val Loss: 1.3270\n",
      "Epoch: 17/20... Step: 2290... Loss: 1.2838... Val Loss: 1.3269\n",
      "Epoch: 17/20... Step: 2300... Loss: 1.2495... Val Loss: 1.3273\n",
      "Epoch: 17/20... Step: 2310... Loss: 1.2775... Val Loss: 1.3280\n",
      "Epoch: 17/20... Step: 2320... Loss: 1.2695... Val Loss: 1.3268\n",
      "Epoch: 17/20... Step: 2330... Loss: 1.2718... Val Loss: 1.3270\n",
      "Epoch: 17/20... Step: 2340... Loss: 1.2879... Val Loss: 1.3224\n",
      "Epoch: 17/20... Step: 2350... Loss: 1.2783... Val Loss: 1.3215\n",
      "Epoch: 17/20... Step: 2360... Loss: 1.2817... Val Loss: 1.3281\n",
      "Epoch: 18/20... Step: 2370... Loss: 1.2608... Val Loss: 1.3270\n",
      "Epoch: 18/20... Step: 2380... Loss: 1.2624... Val Loss: 1.3261\n",
      "Epoch: 18/20... Step: 2390... Loss: 1.2686... Val Loss: 1.3252\n",
      "Epoch: 18/20... Step: 2400... Loss: 1.2842... Val Loss: 1.3220\n",
      "Epoch: 18/20... Step: 2410... Loss: 1.2883... Val Loss: 1.3235\n",
      "Epoch: 18/20... Step: 2420... Loss: 1.2683... Val Loss: 1.3167\n",
      "Epoch: 18/20... Step: 2430... Loss: 1.2689... Val Loss: 1.3228\n",
      "Epoch: 18/20... Step: 2440... Loss: 1.2615... Val Loss: 1.3214\n",
      "Epoch: 18/20... Step: 2450... Loss: 1.2485... Val Loss: 1.3196\n",
      "Epoch: 18/20... Step: 2460... Loss: 1.2705... Val Loss: 1.3173\n",
      "Epoch: 18/20... Step: 2470... Loss: 1.2631... Val Loss: 1.3193\n",
      "Epoch: 18/20... Step: 2480... Loss: 1.2565... Val Loss: 1.3162\n",
      "Epoch: 18/20... Step: 2490... Loss: 1.2358... Val Loss: 1.3121\n",
      "Epoch: 18/20... Step: 2500... Loss: 1.2526... Val Loss: 1.3174\n",
      "Epoch: 19/20... Step: 2510... Loss: 1.2563... Val Loss: 1.3164\n",
      "Epoch: 19/20... Step: 2520... Loss: 1.2574... Val Loss: 1.3183\n",
      "Epoch: 19/20... Step: 2530... Loss: 1.2677... Val Loss: 1.3159\n",
      "Epoch: 19/20... Step: 2540... Loss: 1.2873... Val Loss: 1.3153\n",
      "Epoch: 19/20... Step: 2550... Loss: 1.2442... Val Loss: 1.3145\n",
      "Epoch: 19/20... Step: 2560... Loss: 1.2583... Val Loss: 1.3115\n",
      "Epoch: 19/20... Step: 2570... Loss: 1.2386... Val Loss: 1.3097\n",
      "Epoch: 19/20... Step: 2580... Loss: 1.2648... Val Loss: 1.3118\n",
      "Epoch: 19/20... Step: 2590... Loss: 1.2351... Val Loss: 1.3113\n",
      "Epoch: 19/20... Step: 2600... Loss: 1.2398... Val Loss: 1.3119\n",
      "Epoch: 19/20... Step: 2610... Loss: 1.2502... Val Loss: 1.3090\n",
      "Epoch: 19/20... Step: 2620... Loss: 1.2352... Val Loss: 1.3129\n",
      "Epoch: 19/20... Step: 2630... Loss: 1.2391... Val Loss: 1.3044\n",
      "Epoch: 19/20... Step: 2640... Loss: 1.2505... Val Loss: 1.3103\n",
      "Epoch: 20/20... Step: 2650... Loss: 1.2529... Val Loss: 1.3114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/20... Step: 2660... Loss: 1.2564... Val Loss: 1.3095\n",
      "Epoch: 20/20... Step: 2670... Loss: 1.2630... Val Loss: 1.3056\n",
      "Epoch: 20/20... Step: 2680... Loss: 1.2465... Val Loss: 1.3055\n",
      "Epoch: 20/20... Step: 2690... Loss: 1.2493... Val Loss: 1.3060\n",
      "Epoch: 20/20... Step: 2700... Loss: 1.2494... Val Loss: 1.2992\n",
      "Epoch: 20/20... Step: 2710... Loss: 1.2243... Val Loss: 1.3050\n",
      "Epoch: 20/20... Step: 2720... Loss: 1.2210... Val Loss: 1.3046\n",
      "Epoch: 20/20... Step: 2730... Loss: 1.2173... Val Loss: 1.3026\n",
      "Epoch: 20/20... Step: 2740... Loss: 1.2145... Val Loss: 1.3058\n",
      "Epoch: 20/20... Step: 2750... Loss: 1.2242... Val Loss: 1.3055\n",
      "Epoch: 20/20... Step: 2760... Loss: 1.2232... Val Loss: 1.2985\n",
      "Epoch: 20/20... Step: 2770... Loss: 1.2532... Val Loss: 1.3024\n",
      "Epoch: 20/20... Step: 2780... Loss: 1.2760... Val Loss: 1.3004\n"
     ]
    }
   ],
   "source": [
    "#Setting the training hyperparameters\n",
    "batch_size = 128\n",
    "seq_length = 100\n",
    "n_epochs = 20\n",
    "\n",
    "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apparent-bailey",
   "metadata": {},
   "source": [
    "### Saving checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cross-income",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'char_rnn_1.net'\n",
    "\n",
    "checkpoint = {'n_hidden':net.n_hidden,\n",
    "              'n_layers':net.n_layers,\n",
    "              'state_dict':net.state_dict(),\n",
    "              'tokens':net.chars}\n",
    "\n",
    "with open(model_name,'wb') as f:\n",
    "    torch.save(checkpoint,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "violent-exception",
   "metadata": {},
   "source": [
    "## Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "royal-prague",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, top_k=None):\n",
    "    '''\n",
    "    Given a character, predict the next character\n",
    "    Returns the predicted character and hidden state.\n",
    "    '''\n",
    "    \n",
    "    #Tensor inputs\n",
    "    x = np.array([[net.char2int[char]]])\n",
    "    x = one_hot_encode(x,len(net.chars))\n",
    "    inputs = torch.from_numpy(x)\n",
    "    \n",
    "    if train_on_gpu:\n",
    "        inputs = inputs.cuda()\n",
    "        \n",
    "    #Detach hidden state from history\n",
    "    h = tuple([each.data for each in h])\n",
    "    \n",
    "    #Get the output from the model\n",
    "    out, h = net(inputs,h)\n",
    "    \n",
    "    #Get the character probabilities\n",
    "    p = F.softmax(out, dim=1).data\n",
    "    if train_on_gpu:\n",
    "        p = p.cpu()\n",
    "    \n",
    "    #Get top characters\n",
    "    if top_k is None:\n",
    "        top_ch = np.arange(len(net.chars))\n",
    "    else:\n",
    "        p, top_ch = p.topk(top_k)\n",
    "        top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "    #Selecting the most likely character with some element of randomness\n",
    "    p = p.numpy().squeeze()\n",
    "    char = np.random.choice(top_ch,p=p/p.sum())\n",
    "    \n",
    "    return net.int2char[char], h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continent-knowing",
   "metadata": {},
   "source": [
    "### Priming and generating text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "enhanced-trading",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(net, size, prime=\"The\", top_k=None):\n",
    "    if train_on_gpu:\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    \n",
    "    net.eval()\n",
    "    \n",
    "    #Firstly run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "        \n",
    "    chars.append(char)\n",
    "    \n",
    "    for ii in range(size):\n",
    "        char,h = predict(net,chars[-1],h,top_k=top_k)\n",
    "        chars.append(char)\n",
    "    \n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "billion-harrison",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character in\n",
      "the druving of the belicitures, and the\n",
      "position of this soul.\n",
      "\n",
      "\"Well, I don't believe that he, I'm not\n",
      "struck her to her.\"\n",
      "\n",
      "\"It mean this from her, and I care to do you there.\" \"I can't believe that the misery and take the sort,\"\n",
      "he added.\n",
      "\n",
      "\"Well, when you won't see it to the sound of maticusion,\" answered the praceicing at one and sighing,\n",
      "and he found she was\n",
      "so carried out\n",
      "that the paters stopped the princess to take\n",
      "a long while any acquaintances.\n",
      "\n",
      "\"I don't know what how it seems, and I can't come...\"\n",
      "\n",
      "\"Oh, I what are you getting up on that in other, there's nothing or even this studant\n",
      "of a conversation, that he didn't know, I shall, have you tell\n",
      "her and treating that something.\"\n",
      "\n",
      "\"Oh, you can't believe the same won'e for you are than in her tenstened and my hostess,\" the most\n",
      "figure was at a mander, and as though seemed to the conscetuation. The princess were\n",
      "concentrated for his work, he went to\n",
      "below his father, with the proforming.\n",
      "\n",
      "\"I can't go on anything. You want a man\n"
     ]
    }
   ],
   "source": [
    "print(sample(net,1000,\"Character\",top_k=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-garden",
   "metadata": {},
   "source": [
    "### Loading a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "careful-glance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Here we are loading a pre-trained model 'char_rnn_1.net' that trained on 20 epochs\n",
    "with open('char_rnn_1.net','rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "\n",
    "loaded_model = CharRNN(checkpoint['tokens'],checkpoint['n_hidden'],checkpoint['n_layers'])\n",
    "loaded_model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specified-alexandria",
   "metadata": {},
   "source": [
    "### Some examples of working of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "editorial-scottish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aditya with\n",
      "a bed world, and a clear to say\n",
      "in the belt and tracking on his bed work, where he was, the stall of the peasants was to stray the position with the\n",
      "creature than her head, and as\n",
      "he did not\n",
      "see her son so as to go and take the death, and\n",
      "while the sisters in\n",
      "the children telled it all to the most property, the course were not storting any struggle.\n",
      "\n",
      "\"I can't step my arm, always.\"\n",
      "\n",
      "\"Yes, it's an one when, you do you\n",
      "then, which has\n",
      "been taken the steps, and then you will go\n",
      "away, but I\n",
      "have not the\n",
      "motions, and this.\"\n",
      "\n",
      "\"I am all that too. Would you complete my sensoler,\" said Stepan Arkadyevitch. \"What has the mistakes, but the\n",
      "man as how if there?\"\n",
      "\n",
      "\"I should not be defined! And that I went to him,\" said Alexey Alexandrovitch as he had not been\n",
      "thought of that time\n",
      "that. The countess with all the\n",
      "close with the condlachion, and\n",
      "said alought.\n",
      "\n",
      "\"I'll believe you. And his way we have a gentrement and the studing of her.\"\n",
      "\n",
      "Levin smiled so is to think of the point the comparishing of her an angry, and set to him, and went out to her\n",
      "stall. The patt he was at the match and higher one of\n",
      "her. Taking out a boy was corruded of the steps and had still more than ever in the face, together the contrary had\n",
      "not as a minitate on the\n",
      "fact, and trying to came into what\n",
      "should be askance, and a mere had through all\n",
      "himself. But he was so make the carriage transition to the\n",
      "parts of the province. She could not help. Through\n",
      "this moment all he saw the doctor had\n",
      "thinking of his face.\n",
      "\n",
      "\"Where is nothing but the conversation? There's a course of it out into his man, and the conversation, and what it should have been from my acquaintance than the sing man, and this if I have\n",
      "taken a carriage,\n",
      "and that this was the man, or only the measing of her soul,\" he said, and so as to help suddenly.\n",
      "\n",
      "\"I have seen them, when you doesn't come over.\" \n",
      "\"Yashvin, and he has sere them three\n",
      "finishing, the most faith, and would hear a lund. I'll came to the coar door of any more of my account one, there's nothand of the soul at all, and I cannot be an idea.\"\n",
      "\n",
      "\"Ah! you won't? I ask the passion, and\n",
      "that we've come, there are surraired.\"\n",
      "\n",
      "\"Oh, what work I am going tomater, that I'm talking there.\"\n",
      "\n",
      "\"Only what it is?... If as I could nevar do it of this second, what I don't know\n",
      "you,\n",
      "I'm a great select money.\"\n",
      "\n",
      "\"What did you see is in that thing, though? What do\n",
      "you were\n",
      "not a method of the children. Well, your same doctor was such as so machine.\"\n",
      "\n",
      "\"All, then's an offer and such a cart of something, at it as, he has so much a commassion, and shall\n",
      "think that I have no\n",
      "self-confusion, that I can't come to him, I should be the\n",
      "position, to the matter all that was\n",
      "to the\n",
      "most tired and an has standed in... Sergey Ivanovitch, the constitution of the same.\"\n",
      "\n",
      "\"Yes, you can say so in this contragy out of the carriage,\" said Sergey Ivanovitch. \"I am true, a difference, with you, and won't care all. That was it is\n",
      "to say it's\n",
      "stupid, with her.\"\n",
      "\n",
      "\"It's a complete, they could not be too,\" she s\n"
     ]
    }
   ],
   "source": [
    "print(sample(loaded_model,3000,top_k=5,prime=\"Aditya \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "plastic-insurance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Something, and the signification, and some the princess, the same thing was sitting out on\n",
      "a lone of his wife, he had seen them, as they had to do was some of the stard of the paress of his face at the same. The could never be supersisted at his brother, and so\n",
      "they are no more at their carriage, and\n",
      "her husband was a calm there were the partity, and to be discovered. And seemed to him what to see him. The profoss of the conscience was no disappearance to the same sort of sort of his brother. And the point were a stern and this feeling of\n",
      "heart. They had nothing to him to see the meadow and happy, and something would have the children, and he was so all that she was\n",
      "not still, but there was no often as something than he\n",
      "could not care interested\n",
      "and have but her face. He was a standing to her and\n",
      "said, the profoss, and had brought her the streems of the part of his study.\"\n",
      "\n",
      "\"Where is some of a complex attitude on,\" he added, straight on the painter and sat the same\n",
      "strenct, to be delighted at his best taken of the\n",
      "carriage and still to be\n",
      "said in the sort.\n",
      "\n",
      "Anna smiled.\n",
      "\n",
      "\"Yes, I wore all about. I am so strength of the sone.\n",
      "\n",
      "\"What a single deal thinks will answer the common with the mother,\" said Levin, with a station.\n",
      "\n",
      "And the sight of the\n",
      "princess was not\n",
      "a start, the sight of the\n",
      "strentth and that sensation that they could not be the\n",
      "country, who had been always and a sord of the\n",
      "same side. He was not a children and that he was so much that the position in the most coming towards\n",
      "the strain and sone to her and she said her father.\n",
      "\n",
      "Alexey Alexandrovitch, as he saw that he had seemed to have saying that the simple to him.\n",
      "\n",
      "The profits had to bring the strange timad was the property. All the princess had\n",
      "been a commin of all, the same time to\n",
      "the simple time the princess,\" said Levin.\n",
      "\n",
      "\"Yes, you want to say to them along, but it's a children arouse than the country.\"\n",
      "\n",
      "The stards were the standing, the same thing of table, the princess, a stepped to him, and taken the\n",
      "commission.\n",
      "\n",
      "\"Well, well, will be anything to be so, take the strain and saying, and then are so marriage.\"\n",
      "\n",
      "\"Where'll you take the same, I'm all about you to me, to\n",
      "be such a man. I can't be all to see her.\"\n",
      "\n",
      "\"Well, and we said it, that I should be a prettical tall,\" answered, said to the princess.\n",
      "\n",
      "They were a fearful to steps the same to them on a contemptuous conversation to\n",
      "his father, and he was not at this steed, though, and to\n",
      "the standand and to be askanted in the same time, the strange face was\n",
      "standing and say the same time to tee to the charming of all the sense of the\n",
      "sound of the same steps to\n",
      "him with his hand to her at the country and so\n",
      "such a state of some thought of the same side. He still heard the princess, and to be an implession of the same and still there was nothing of an\n",
      "old man to talk to the sates. He stood at him. He had seen her son. \"I am not a party of him. Tell you with the man of the same, what\n",
      "we say,\" he said to her and still meeting her head.\n",
      "\n",
      "\"I had been a proposed is\n",
      "always and mother into the second, and there's\n",
      "all this sight, and to say it,\n",
      "to see her to see you, and, I shall see her.\" He said to him. He heard a single smile.\n",
      "\n",
      "\"Yes, that's to be disagreeable to\n",
      "me in the same and the possibility of the princess.\"\n",
      "\n",
      "\"Yes, there's nothing all to the committee. And that in the children hopes and so take all the\n",
      "same stand of them. When, what are\n",
      "you are such a moment.\"\n",
      "\n",
      "\"What is it in that, I see how it was that in the same as that was a side on the prevalious of her, and there are sure all at\n",
      "once. All that is anything that he was\n",
      "standing and went\n",
      "to say to her,\" he said, and\n",
      "said that the conversation was a sigh for her. He had not been the same sister, then the significant of the counters, and the satisfaction with the\n",
      "charme to him, and the\n",
      "prince was a man was standing at the sould and so serious from the countess. \"Yes, yes, I'm all\n",
      "all seeing them. And here is the\n",
      "priest and has been an official for anyone, the meadors, and there were so talking and sort and the comminstion tired.\"\n",
      "\n",
      "\"It's that,\" said Sergey\n",
      "Ivanovitch, as he was said to her. \"It's all this will be an imalish on his foreshand,\" he said.\n",
      "\"And there, way!\" she said, and that sometimes were a standing at the station. \"Yes, yes are the carriage.\n",
      "I didn't consequent him, and I cannot believe her in the country, but I was all this, and too all to the\n",
      "principles were\n",
      "something in the peasants.\"\n",
      "\n",
      "\"I don't say it, I'll see you.\"\n",
      "\n",
      "\"I have not thinking that I'm always been and there would nother. I have nothing but the sound of his\n",
      "mistress. All,\n",
      "that we would\n",
      "be a capable of courtes of me to say the charming the children. And then I see her, but it say as that there was some to her, and I should\n",
      "natural,\n",
      "anyway,\" he added, and took a conversating, and saw his face and straight on his hands, and he had been\n",
      "sat down to him as he\n",
      "was staying and the\n",
      "same the state, his face was the country. His shade\n",
      "of his start with the property of the contrary.\n",
      "\n",
      "\"Yes,\n"
     ]
    }
   ],
   "source": [
    "print(sample(loaded_model,5000,top_k=3,prime=\"Something\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "capable-marijuana",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DLly and the forest. This studic and her steps, they would have been successful interests to be\n",
      "there, there are saling the personary of the coachman. But\n",
      "the cart and calm than he had\n",
      "said to his heart all all seeing\n",
      "him.\n",
      "\n",
      "\"It'll go on, I did not know,\" he added, smiling.\n",
      "\n",
      "\"You mean and must go,\" she said, and that he was so much\n",
      "to his wife's stucial to\n",
      "the proversions of her, he was not than\n",
      "examinisatiss at the position of the possibility of the compering of it, but\n",
      "she was trauthing a strange feeling of contemptuous and such an every table and still the fact was not by now. And when she could not hind this sheeps.\n",
      "\n",
      "\"Ah!'s as you don't come at a moment and her to me.\"\n",
      "\n",
      "\"Oh, no, what is she? As though I\n",
      "should hear her? The peasants there was so in a subject.\"\n",
      "\n",
      "\"Yes,\" thought Serguy Ivanovitch and his honest he did not left him\n",
      "and who had been\n",
      "their some\n",
      "smile.\n",
      "\n",
      "\"I should not say a silence.\"\n",
      "\n",
      "\"Yes, in the ceuration, to be somewhere,\" he said, letting in,\n",
      "and a little crushed hands and sat something, took his forest.\n",
      "\n",
      "\"The satisfied were to be, which you'd give me on\n",
      "the man.\"\n",
      "\n",
      "\"No; well, it's to supprise.\"\n",
      "\n",
      "And the sonn, where he would say.\n",
      "\n",
      "\"Yes, I walk, and I do.\"\n",
      "\n",
      "\"Oh! I did not believe that you do there's not as you will, say that I say. To be disentaying their pate,\n",
      "and I should be in his ball.\"\n",
      "\n",
      "\"I were no distantication.\"\n",
      "\n",
      "\"I see that I've come to the same, and\n",
      "the day if I have bight about my and the clerk word and think of the ball in what they would not give my figure that you cannot come.\"\n",
      "\n",
      "\"You do it about how is to day, then you are nearing\n",
      "you,\" said Levin.\n",
      "\n",
      "Her life was at the same case of the memories, but the crime, and he still said, and at the same down.\n",
      "\n",
      "\"Well, I am as you don't know how to be the time with you?\"\n",
      "\n",
      "Alexey Alexandrovitch said his head,\n",
      "\"but was an acquaintance of the pretenderest\n",
      "of horrow and a salaray of my a single\n",
      "arm times. And steps it was a peculiar truth. I have been\n",
      "standing the streak of a colfer of the country almost, t\n"
     ]
    }
   ],
   "source": [
    "#Sample using loaded model\n",
    "print(sample(loaded_model,2000,top_k=5,prime=\"DL\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fluid-kansas",
   "metadata": {},
   "source": [
    "# END OF NOTEBOOK"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
